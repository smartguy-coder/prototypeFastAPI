version: '3.8'

services:
  documentation:
    image: squidfunk/mkdocs-material:latest
    container_name: mkdocs
    hostname: mkdocs
    command: serve --dev-addr=0.0.0.0:8010 --watch-theme
    restart: unless-stopped
    ports:
      - "8010:8010"
    volumes:
      - ./documentation:/docs:ro
    stdin_open: true
    tty: true
    networks:
      - main_network

  master-backend-api:
    build:
      dockerfile: Dockerfile
      context: ./master_backend_api
    container_name: master-backend-api
    hostname: master-backend-api
    restart: always
    env_file: .env
    healthcheck:
      test: curl -f http://localhost:10000/docs || exit 1
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 10s
    volumes:
      - ./master_backend_api/app:/app
      - uds_socket:/app/tmp_uds
    ports:
      - "10000:10000"
    command: |
      sh -c "
        alembic upgrade head && \
    
        uvicorn main:app --port=10000 --host=0.0.0.0 --workers 1 --reload --no-access-log
      "
    #        uvicorn main:app --uds=/app/tmp_uds/master-backend.sock --workers 1 --reload --no-access-log &
    #
    #        until [ -S /app/tmp_uds/master-backend.sock ]; do
    #          echo 'Чекаю на сокет...'
    #          sleep 0.1
    #        done
    #
    # gunicorn -w 4 -b 0.0.0.0:10000 -k uvicorn.workers.UvicornWorker main:app
    networks:
      - main_network
      - postgres_network
      - redis_network
      - rabbitmq_network
    depends_on:
      postgres_database:
        condition: service_healthy
      redis:
        condition: service_started
      rabbitmq:
        condition: service_healthy
      notification_service:
        condition: service_started
      s3:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy

  web-jinja:
    build:
      dockerfile: Dockerfile
      context: ./web_jinja
    container_name: web-jinja
    restart: unless-stopped
    env_file: .env
    healthcheck:
      test: curl -f http://localhost:15000/docs || exit 1
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 10s
    volumes:
      - ./web_jinja/app:/app
      - uds_socket:/app/tmp_uds
    ports:
      - "15000:15000"
    command: |
      sh -c "
        uvicorn main:app --port=15000 --host=0.0.0.0 --workers 1 --reload --no-access-log
      "
    #        uvicorn main:app --uds=/app/tmp_uds/master-backend.sock --workers 1 --reload --no-access-log &
    #
    #        until [ -S /app/tmp_uds/master-backend.sock ]; do
    #          echo 'Чекаю на сокет...'
    #          sleep 0.1
    #        done
    #

    # gunicorn -w 4 -b 0.0.0.0:15000 -k uvicorn.workers.UvicornWorker main:app
    networks:
      - main_network
    depends_on:
      master-backend-api:
        condition: service_healthy



  notification_service:
    build:
      dockerfile: Dockerfile
      context: ./notification_service
    container_name: notification_service
    restart: always
    env_file: .env
    volumes:
      - ./notification_service/app:/app
    command: |
      sh -c "
        python /app/main.py
      "
    networks:
      - rabbitmq_network
      - main_network
    depends_on:
      rabbitmq:
        condition: service_healthy

  postgres_database:
    image: postgres:16
    container_name: postgres_database
    restart: always
    env_file: .env
    ports:
      - "${POSTGRES_PORT}:${POSTGRES_PORT}"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres_user -d postgres_db || exit 1" ]  # pg_isready - is a postgres util
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 4G
    networks:
      - main_network
      - postgres_network

  pgadmin:
    container_name: pgadmin_container
    image: dpage/pgadmin4:9.0
    env_file: .env
    volumes:
      - pgadmin-data:/var/lib/pgadmin
    ports:
      - "${PGADMIN_PORT}:80"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
    networks:
      - postgres_network
    depends_on:
      postgres_database:
        condition: service_healthy

  redis:
    image: redis:7.0.8-alpine
    container_name: redis
    restart: unless-stopped
    env_file: .env
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: [ "redis-server", "--appendonly", "no", "--maxmemory", "10mb", "--maxmemory-policy", "allkeys-lru" ]
    healthcheck:
      test: ['CMD', "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - main_network
      - redis_network

  redis-insight:
    image: redis/redisinsight:latest
    container_name: redis-insight
    restart: unless-stopped
    ports:
      - "5540:5540"
    volumes:
      - redis-insight:/data
    networks:
      - redis_network
    depends_on:
      redis:
        condition: service_healthy

  rabbitmq:
    image: rabbitmq:3-management-alpine
    hostname: ${RABBITMQ_HOSTNAME}
    container_name: ${RABBITMQ_CONTAINER_NAME}
    restart: always
    env_file:
      - .env
    ports:
      - "${RABBITMQ_AMQP_PORT}:5672"
      - "${RABBITMQ_UI_PORT}:15672"
    environment:
      RABBITMQ_DEFAULT_USER: "${RABBITMQ_DEFAULT_USER}"
      RABBITMQ_DEFAULT_PASS: "${RABBITMQ_DEFAULT_PASS}"
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    networks:
      - rabbitmq_network
      - main_network
    healthcheck:
      test: rabbitmq-diagnostics -q check_running
      interval: 10s
      timeout: 10s
      retries: 5

  s3:
    image: bitnami/minio
    container_name: s3
    restart: unless-stopped
    env_file:
      - .env
    environment:
      MINIO_ROOT_USER: "${S3_ACCESS_KEY}"
      MINIO_ROOT_PASSWORD: "${S3_SECRET_KEY}"
      MINIO_DEFAULT_BUCKETS: "${S3_DEFAULT_BUCKET_NAME}:public"
    volumes:
      - s3-data:/data
    ports:
      - "${S3_PORT}:${S3_PORT}"
      - "9001:9001"
    networks:
      - main_network
    healthcheck:
     test: ["CMD", "curl", "-f", "http://localhost:${S3_PORT}/minio/health/live"]
     interval: 30s
     timeout: 20s
     retries: 3

  nginx:
    image: nginx:alpine
    container_name: nginx
    restart: always
    ports:
      - '80:80'
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf
    networks:
      - main_network

  # metrics
#  https://youtu.be/WWzl53ObYvo?si=gGxzdKIBQg0f_y8Q
#  https://youtu.be/x3bSuGH8R28?si=L2pJnUHAtYBPcxTq
#  https://youtu.be/1RpbstvgnVE?si=BnZ97w2dmcPgQ98-
  prometheus:
    image: prom/prometheus
    restart: always
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus_data:/prometheus  # Директори для даних Prometheus
      - ./logs/prometheus.log:/var/log/prometheus.log  # Логи Prometheus
    logging:
      driver: "json-file"
#      driver: "local"
#      driver: "syslog"
      options:
        max-size: "10m"  # Обмеження розміру файлу
        max-file: "3"     # Зберігати 3 файли
#    command:
#      - '--config.file=/etc/prometheus/prometheus.yml'
    ports:
      - "9090:9090"
    networks:
      - main_network


  grafana:
    # default login -> admin, password -> admin
    #https://chatgpt.com/share/67c420f0-cc84-8004-84b2-03b78e2d6e50
    image: grafana/grafana
    ports:
      - "3000:3000"
    networks:
      - main_network
    depends_on:
      - prometheus

  # logs
#
#  logstash:
#    image: docker.elastic.co/logstash/logstash:8.5.3
#    container_name: logstash
#    volumes:
#      - ./logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
#    depends_on:
#      - elasticsearch
#    ports:
#      - "5000:5000"
#      - "9600:9600"  # API Logstash для моніторингу http://127.0.0.1:9600/
#    environment:
#      - "LS_JAVA_OPTS=-Xms256m -Xmx256m"
#
#    networks:
#      - main_network

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.7.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - ELASTIC_USERNAME=elastic
      - ELASTIC_PASSWORD=changeme
    ports:
      - "9200:9200"
    volumes:
      - elastic_data:/usr/share/elasticsearch/data
    networks:
      - main_network
    healthcheck:
          test: ["CMD-SHELL", "curl -fsSL http://localhost:9200 || exit 1"]
          interval: 30s
          timeout: 10s
          retries: 5

  kibana:
    # maybe add https://youtu.be/bKc3aAttmKM?si=lyw1xkknFMbgSWjf from here
    # 127.0.0.1:5601
    # management-stack management-
    image: docker.elastic.co/kibana/kibana:8.7.0
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
#      - ELASTICSEARCH_USERNAME=elastic
#      - ELASTICSEARCH_PASSWORD=changeme
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - main_network

  # testing (2 nodes)
  master_test_node:
    image: locustio/locust
    ports:
     - "8089:8089"
    volumes:
      - ./locust_testing:/mnt/locust
    command: -f /mnt/locust/locustfile.py --master -H http://master-backend-api:10000
    networks:
      - main_network

  worker_test_node:
    image: locustio/locust
    volumes:
      - ./locust_testing:/mnt/locust
    command: -f /mnt/locust/locustfile.py --worker --master-host master_test_node
    networks:
      - main_network


volumes:
  postgres-data:
    external: false
  pgadmin-data:
  redis-data:
  redis-insight:
  rabbitmq-data:
  s3-data:
  elastic_data:
  uds_socket:
    driver_opts:
      type: tmpfs
      device: tmpfs

networks:
  main_network:
    driver: bridge
  postgres_network:
  redis_network:
  rabbitmq_network:
